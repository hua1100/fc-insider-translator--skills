#!/usr/bin/env python3
"""
ç¿»è¯‘å¯¹ç…§è¡¨ç”Ÿæˆå™¨

ä» Markdown è¡¨æ ¼å’Œæ–°è¯‘æ–‡ç”Ÿæˆæ–°æ—§ç¿»è¯‘å¯¹ç…§è¡¨
è¾“å‡ºæ ¼å¼ä¸ update_fc_insider_v3.py å…¼å®¹

å·¥ä½œæµç¨‹ï¼š
1. è¯»å–ä» Word æå–çš„ Markdown è¡¨æ ¼
2. è¯»å–æ–°è¯‘æ–‡ï¼ˆå¯ä»¥æ˜¯çº¯æ–‡æœ¬ã€JSON æˆ– AI è¾“å‡ºï¼‰
3. ç”Ÿæˆ translations.json å¯¹ç…§è¡¨
4. æä¾›éªŒè¯å’Œé¢„è§ˆåŠŸèƒ½
"""

import json
import argparse
import re
from pathlib import Path
from typing import List, Dict, Optional


def load_markdown_table(md_path: str) -> List[Dict[str, str]]:
    """
    ä» Markdown åŠ è½½è¡¨æ ¼æ•°æ®

    Returns:
        List of dicts with keys: segment_id, status, source, target
    """
    with open(md_path, 'r', encoding='utf-8') as f:
        lines = f.readlines()

    rows = []
    in_table = False

    for line in lines:
        line = line.strip()

        if line.startswith('|') and line.endswith('|'):
            # è·³è¿‡åˆ†éš”ç¬¦
            if set(line.replace('|', '').replace('-', '').strip()) == set():
                in_table = True
                continue

            # è·³è¿‡è¡¨å¤´
            if 'Segment ID' in line or 'segment_id' in line.lower():
                in_table = True
                continue

            if in_table:
                cells = [cell.strip() for cell in line.split('|')[1:-1]]
                if len(cells) >= 4:
                    rows.append({
                        'segment_id': cells[0],
                        'status': cells[1],
                        'source': cells[2],
                        'target': cells[3]
                    })

    return rows


def is_placeholder_row(text: str) -> bool:
    """
    åˆ¤æ–­æ˜¯å¦ä¸ºå ä½ç¬¦è¡Œ

    å ä½ç¬¦è¡Œçš„ç‰¹å¾ï¼š
    - ä¸»è¦ç”± <æ•°å­—/> æ ‡è®°ç»„æˆ
    - å¯èƒ½åŒ…å«å°‘é‡å›ºå®šæ–‡æœ¬ï¼ˆå¦‚"åœ¨ç¬¬"ã€"é "ï¼‰
    - ä¾‹å¦‚: "<0/>"åœ¨ç¬¬ <1/> é , "<2/>", ç¬¬ <12/> é 

    Returns:
        True if the text is primarily placeholders
    """
    # ç§»é™¤æ‰€æœ‰å ä½ç¬¦
    without_placeholders = re.sub(r'[<"]?\d+/?[>"]?', '', text)
    # ç§»é™¤å¼•å·
    without_placeholders = re.sub(r'["""\'\'<>]', '', without_placeholders)
    # ç§»é™¤å¸¸è§çš„è¿æ¥è¯
    without_placeholders = re.sub(r'(åœ¨ç¬¬|é |on page|page)', '', without_placeholders, flags=re.IGNORECASE)
    # ç§»é™¤ç©ºç™½
    without_placeholders = without_placeholders.strip()

    # å¦‚æœç§»é™¤å ä½ç¬¦åå‰©ä½™å†…å®¹å¾ˆå°‘ï¼Œè®¤ä¸ºæ˜¯å ä½ç¬¦è¡Œ
    if len(without_placeholders) <= 3:
        return True

    # æ£€æŸ¥æ˜¯å¦åŒ…å«å¤§é‡å ä½ç¬¦æ ‡è®°
    placeholder_count = len(re.findall(r'<\d+/>', text))
    if placeholder_count >= 2:
        # å¦‚æœæœ‰2ä¸ªæˆ–æ›´å¤šå ä½ç¬¦ï¼Œä¸”æ€»é•¿åº¦å¾ˆçŸ­
        if len(text) <= 30:
            return True

    return False


def filter_placeholder_rows(rows: List[Dict[str, str]], verbose: bool = False) -> List[Dict[str, str]]:
    """
    è¿‡æ»¤æ‰å ä½ç¬¦è¡Œ

    Args:
        rows: è¡¨æ ¼è¡Œåˆ—è¡¨
        verbose: æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯

    Returns:
        è¿‡æ»¤åçš„è¡Œåˆ—è¡¨
    """
    filtered = []
    skipped = []

    for row in rows:
        target_text = row['target']

        if is_placeholder_row(target_text):
            skipped.append(row)
        else:
            filtered.append(row)

    if verbose:
        print(f"\nå ä½ç¬¦è¿‡æ»¤:")
        print(f"  æ€»è¡Œæ•°: {len(rows)}")
        print(f"  ä¿ç•™: {len(filtered)}")
        print(f"  è·³è¿‡: {len(skipped)}")

        if skipped:
            print(f"\nè·³è¿‡çš„å ä½ç¬¦è¡Œï¼ˆå‰10ä¸ªï¼‰:")
            for i, row in enumerate(skipped[:10], 1):
                print(f"    {i}. {row['segment_id']}: {row['target'][:50]}")

    return filtered


def load_new_translations(input_path: str, format: str = 'auto') -> Dict[str, str]:
    """
    åŠ è½½æ–°è¯‘æ–‡

    æ”¯æŒæ ¼å¼ï¼š
    - json: {"segment_id": "new_text", ...}
    - text: æŒ‰è¡Œåˆ†å‰²çš„è¯‘æ–‡ï¼ˆä¸è¡¨æ ¼è¡Œå¯¹åº”ï¼‰
    - auto: è‡ªåŠ¨æ£€æµ‹
    """
    with open(input_path, 'r', encoding='utf-8') as f:
        content = f.read()

    # è‡ªåŠ¨æ£€æµ‹æ ¼å¼
    if format == 'auto':
        if content.strip().startswith('{'):
            format = 'json'
        else:
            format = 'text'

    if format == 'json':
        data = json.loads(content)
        # æ”¯æŒå¤šç§ JSON æ ¼å¼
        if isinstance(data, dict) and 'translations' in data:
            # æ ¼å¼ 1: {"translations": [{"segment_id": "...", "text": "..."}, ...]}
            return {
                item['segment_id']: item.get('text', item.get('new_text', ''))
                for item in data['translations']
            }
        elif isinstance(data, dict):
            # æ ¼å¼ 2: {"segment_id": "new_text", ...}
            return data
        else:
            raise ValueError("ä¸æ”¯æŒçš„ JSON æ ¼å¼")

    elif format == 'text':
        # æ¯è¡Œä¸€ä¸ªè¯‘æ–‡
        lines = [line.strip() for line in content.split('\n') if line.strip()]
        return {str(i): line for i, line in enumerate(lines)}

    else:
        raise ValueError(f"ä¸æ”¯æŒçš„æ ¼å¼: {format}")


def generate_translation_mapping(
    old_table: List[Dict[str, str]],
    new_translations: Dict[str, str],
    match_by: str = 'segment_id'
) -> List[Dict[str, str]]:
    """
    ç”Ÿæˆæ–°æ—§ç¿»è¯‘å¯¹ç…§è¡¨

    Args:
        old_table: ä» Word æå–çš„åŸå§‹è¡¨æ ¼
        new_translations: æ–°è¯‘æ–‡ï¼ˆsegment_id -> new_textï¼‰
        match_by: åŒ¹é…æ–¹å¼ï¼ˆ'segment_id' æˆ– 'index'ï¼‰

    Returns:
        List of translation mappings for update_fc_insider_v3.py
    """
    mappings = []

    for idx, row in enumerate(old_table):
        segment_id = row['segment_id']
        old_text = row['target']

        # åŒ¹é…æ–°è¯‘æ–‡
        if match_by == 'segment_id':
            new_text = new_translations.get(segment_id)
        elif match_by == 'index':
            new_text = new_translations.get(str(idx))
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„åŒ¹é…æ–¹å¼: {match_by}")

        # åªæœ‰å½“æ–°è¯‘æ–‡å­˜åœ¨ä¸”ä¸æ—§è¯‘æ–‡ä¸åŒæ—¶æ‰æ·»åŠ 
        if new_text and new_text != old_text:
            mappings.append({
                'segment_id': segment_id,
                'old_text': old_text,
                'new_text': new_text
            })

    return mappings


def preview_changes(mappings: List[Dict[str, str]], limit: int = 10):
    """
    é¢„è§ˆå˜æ›´

    æ˜¾ç¤ºå‰ N ä¸ªå˜æ›´ä»¥ä¾›äººå·¥éªŒè¯
    """
    print("\n" + "=" * 80)
    print(f"å˜æ›´é¢„è§ˆï¼ˆå‰ {min(limit, len(mappings))} ä¸ªï¼Œå…± {len(mappings)} ä¸ªï¼‰")
    print("=" * 80)

    for i, mapping in enumerate(mappings[:limit], 1):
        seg_id = mapping['segment_id']
        old = mapping['old_text'][:100]  # é™åˆ¶æ˜¾ç¤ºé•¿åº¦
        new = mapping['new_text'][:100]

        print(f"\n[{i}] {seg_id}")
        print(f"  æ—§: {old}{'...' if len(mapping['old_text']) > 100 else ''}")
        print(f"  æ–°: {new}{'...' if len(mapping['new_text']) > 100 else ''}")

    if len(mappings) > limit:
        print(f"\n... è¿˜æœ‰ {len(mappings) - limit} ä¸ªå˜æ›´ï¼ˆçœç•¥æ˜¾ç¤ºï¼‰")


def validate_mappings(mappings: List[Dict[str, str]]) -> bool:
    """
    éªŒè¯å¯¹ç…§è¡¨çš„å®Œæ•´æ€§

    æ£€æŸ¥ï¼š
    - segment_id ä¸ä¸ºç©º
    - new_text ä¸ä¸ºç©º
    - æ²¡æœ‰é‡å¤çš„ segment_id
    """
    if not mappings:
        print("âœ— é”™è¯¯ï¼šå¯¹ç…§è¡¨ä¸ºç©º")
        return False

    seen_ids = set()
    errors = []

    for i, mapping in enumerate(mappings, 1):
        seg_id = mapping.get('segment_id', '').strip()
        new_text = mapping.get('new_text', '').strip()

        if not seg_id:
            errors.append(f"è¡Œ {i}: segment_id ä¸ºç©º")

        if not new_text:
            errors.append(f"è¡Œ {i} ({seg_id}): new_text ä¸ºç©º")

        if seg_id in seen_ids:
            errors.append(f"è¡Œ {i}: segment_id '{seg_id}' é‡å¤")

        seen_ids.add(seg_id)

    if errors:
        print("âœ— éªŒè¯å¤±è´¥:")
        for error in errors[:10]:  # åªæ˜¾ç¤ºå‰ 10 ä¸ªé”™è¯¯
            print(f"  â€¢ {error}")
        if len(errors) > 10:
            print(f"  ... è¿˜æœ‰ {len(errors) - 10} ä¸ªé”™è¯¯")
        return False

    print("âœ“ éªŒè¯é€šè¿‡")
    return True


def main():
    parser = argparse.ArgumentParser(
        description='ç”Ÿæˆæ–°æ—§ç¿»è¯‘å¯¹ç…§è¡¨',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog='''
ç¤ºä¾‹:
  # ä» Markdown è¡¨æ ¼å’Œæ–°è¯‘æ–‡ JSON ç”Ÿæˆå¯¹ç…§è¡¨
  python generate_translation_mapping.py \\
    --markdown table.md \\
    --new-translations new_trans.json \\
    --output translations.json

  # é¢„è§ˆå˜æ›´è€Œä¸ä¿å­˜
  python generate_translation_mapping.py \\
    --markdown table.md \\
    --new-translations new_trans.json \\
    --preview-only

  # ä½¿ç”¨è¡Œç´¢å¼•åŒ¹é…ï¼ˆå½“ segment_id ä¸å¯ç”¨æ—¶ï¼‰
  python generate_translation_mapping.py \\
    --markdown table.md \\
    --new-translations new_trans.txt \\
    --match-by index \\
    --output translations.json
        '''
    )

    parser.add_argument(
        '--markdown',
        required=True,
        help='ä» Word æå–çš„ Markdown è¡¨æ ¼'
    )
    parser.add_argument(
        '--new-translations',
        required=True,
        help='æ–°è¯‘æ–‡æ–‡ä»¶ï¼ˆJSON æˆ–æ–‡æœ¬ï¼‰'
    )
    parser.add_argument(
        '--output',
        default='translations.json',
        help='è¾“å‡ºå¯¹ç…§è¡¨è·¯å¾„ï¼ˆé»˜è®¤ï¼štranslations.jsonï¼‰'
    )
    parser.add_argument(
        '--match-by',
        choices=['segment_id', 'index'],
        default='segment_id',
        help='åŒ¹é…æ–¹å¼ï¼ˆé»˜è®¤ï¼šsegment_idï¼‰'
    )
    parser.add_argument(
        '--format',
        choices=['json', 'text', 'auto'],
        default='auto',
        help='æ–°è¯‘æ–‡æ ¼å¼ï¼ˆé»˜è®¤ï¼šautoï¼‰'
    )
    parser.add_argument(
        '--preview-only',
        action='store_true',
        help='åªé¢„è§ˆå˜æ›´ï¼Œä¸ä¿å­˜æ–‡ä»¶'
    )
    parser.add_argument(
        '--skip-placeholder-filter',
        action='store_true',
        help='è·³è¿‡å ä½ç¬¦è¿‡æ»¤ï¼ˆé»˜è®¤ä¼šè¿‡æ»¤ï¼‰'
    )
    parser.add_argument(
        '--verbose',
        action='store_true',
        help='æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯'
    )

    args = parser.parse_args()

    print("=" * 80)
    print("ç”Ÿæˆç¿»è¯‘å¯¹ç…§è¡¨")
    print("=" * 80)

    # åŠ è½½æ•°æ®
    print(f"\nè¯»å– Markdown è¡¨æ ¼: {args.markdown}")
    old_table = load_markdown_table(args.markdown)
    print(f"âœ“ åŠ è½½ {len(old_table)} è¡Œ")

    # è¿‡æ»¤å ä½ç¬¦è¡Œ
    if not args.skip_placeholder_filter:
        old_table = filter_placeholder_rows(old_table, args.verbose)
        print(f"âœ“ è¿‡æ»¤åä¿ç•™ {len(old_table)} è¡Œï¼ˆè·³è¿‡äº†å ä½ç¬¦è¡Œï¼‰")

    print(f"\nè¯»å–æ–°è¯‘æ–‡: {args.new_translations}")
    new_translations = load_new_translations(args.new_translations, args.format)
    print(f"âœ“ åŠ è½½ {len(new_translations)} ä¸ªè¯‘æ–‡")

    # è‡ªåŠ¨è½¬æ¢ï¼šå¦‚æœæ˜¯ text æ ¼å¼ + segment_id åŒ¹é…ï¼Œè‡ªåŠ¨è½¬æ¢æˆ JSON æ ¼å¼
    if args.match_by == 'segment_id' and isinstance(list(new_translations.keys())[0] if new_translations else '', str) and list(new_translations.keys())[0].isdigit() if new_translations else False:
        print(f"\nğŸ”„ æ£€æµ‹åˆ°çº¯æ–‡æœ¬æ ¼å¼ + segment_id åŒ¹é…æ¨¡å¼")
        print(f"   è‡ªåŠ¨å°†æ–‡æœ¬è½¬æ¢ä¸º JSON æ ¼å¼ï¼ˆæ–‡æœ¬è¡Œ â†’ segment_idï¼‰...")

        # å°†ç´¢å¼•æ˜ å°„è½¬æ¢ä¸º segment_id æ˜ å°„
        text_list = [new_translations[str(i)] for i in range(len(new_translations))]

        if len(text_list) != len(old_table):
            print(f"\nâš ï¸  è­¦å‘Šï¼š")
            print(f"   æ–°ç¿»è¯‘è¡Œæ•°: {len(text_list)}")
            print(f"   è¿‡æ»¤åè¡¨æ ¼è¡Œæ•°: {len(old_table)}")
            if len(text_list) < len(old_table):
                print(f"   âœ— æ–°ç¿»è¯‘è¡Œæ•°ä¸è¶³ï¼è¯·æ£€æŸ¥æ–°ç¿»è¯‘æ–‡ä»¶")
                return 1
            elif len(text_list) > len(old_table):
                print(f"   âš  æ–°ç¿»è¯‘è¡Œæ•°è¿‡å¤šï¼Œå°†åªä½¿ç”¨å‰ {len(old_table)} è¡Œ")
                text_list = text_list[:len(old_table)]

        # è½¬æ¢ä¸º segment_id -> text æ˜ å°„
        converted_translations = {}
        for idx, row in enumerate(old_table):
            if idx < len(text_list):
                converted_translations[row['segment_id']] = text_list[idx]

        new_translations = converted_translations
        print(f"âœ“ è½¬æ¢å®Œæˆï¼š{len(new_translations)} ä¸ªè¯‘æ–‡å·²æ˜ å°„åˆ° segment_id")

        if args.verbose:
            print(f"\nè½¬æ¢ç¤ºä¾‹ï¼ˆå‰3ä¸ªï¼‰:")
            for i, (seg_id, text) in enumerate(list(new_translations.items())[:3], 1):
                print(f"  {i}. {seg_id}: {text[:50]}{'...' if len(text) > 50 else ''}")

    # ç”Ÿæˆå¯¹ç…§è¡¨
    print(f"\nç”Ÿæˆå¯¹ç…§è¡¨ï¼ˆåŒ¹é…æ–¹å¼: {args.match_by}ï¼‰...")
    mappings = generate_translation_mapping(
        old_table,
        new_translations,
        match_by=args.match_by
    )
    print(f"âœ“ ç”Ÿæˆ {len(mappings)} ä¸ªå˜æ›´")

    # éªŒè¯
    if not validate_mappings(mappings):
        return 1

    # é¢„è§ˆ
    preview_changes(mappings)

    # ä¿å­˜
    if not args.preview_only:
        output_data = {'translations': mappings}
        with open(args.output, 'w', encoding='utf-8') as f:
            json.dump(output_data, f, ensure_ascii=False, indent=2)

        print(f"\nâœ“ å¯¹ç…§è¡¨å·²ä¿å­˜: {args.output}")
        print(f"\nä¸‹ä¸€æ­¥:")
        print(f"  python update_fc_insider_v3.py \\")
        print(f"    --unpacked <unpacked_dir> \\")
        print(f"    --translations {args.output} \\")
        print(f"    --rsid <rsid> \\")
        print(f"    --author 'Your Name'")
    else:
        print("\n(é¢„è§ˆæ¨¡å¼ï¼šæœªä¿å­˜æ–‡ä»¶)")

    return 0


if __name__ == '__main__':
    import sys
    sys.exit(main())
