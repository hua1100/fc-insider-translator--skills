# é«˜çº§åŠŸèƒ½è¯¦è§£

æ·±å…¥äº†è§£æ ¸å¿ƒåŠŸèƒ½çš„å·¥ä½œåŸç†å’Œé«˜çº§ç”¨æ³•ã€‚

---

## æ™ºèƒ½åŒ¹é…

ä½¿ç”¨æ–‡æœ¬ç›¸ä¼¼åº¦ç®—æ³•è‡ªåŠ¨é…å¯¹æ–°æ—§ç¿»è¯‘ï¼Œå³ä½¿é¡ºåºå®Œå…¨ä¸ä¸€è‡´ä¹Ÿèƒ½æ­£ç¡®åŒ¹é…ã€‚

### å·¥ä½œåŸç†

#### 1. ç›¸ä¼¼åº¦è®¡ç®—

ç»¼åˆä½¿ç”¨ä¸‰ç§æ–¹æ³•è®¡ç®—ç›¸ä¼¼åº¦ï¼š

```python
def calculate_text_similarity(text1: str, text2: str) -> float:
    # 1. åºåˆ—ç›¸ä¼¼åº¦ (æƒé‡ 50%)
    seq_ratio = SequenceMatcher(None, text1, text2).ratio()

    # 2. å…±åŒå­—ç¬¦æ¯”ä¾‹ (æƒé‡ 20%)
    set1, set2 = set(text1), set(text2)
    char_ratio = len(set1 & set2) / max(len(set1), len(set2))

    # 3. è¯æ±‡é‡å åº¦ (æƒé‡ 30%)
    words1 = set(re.findall(r'[\w]+', text1))
    words2 = set(re.findall(r'[\w]+', text2))
    word_ratio = len(words1 & words2) / max(len(words1), len(words2))

    # åŠ æƒç»¼åˆ
    return (seq_ratio * 0.5) + (char_ratio * 0.2) + (word_ratio * 0.3)
```

**ç¤ºä¾‹**ï¼š
```
æ–‡æœ¬1: "PY26 å·²è‡³ï¼Œä½œç‚ºå…¨çƒæ”¿ç­–è«®è©¢å§”å“¡é ˜å°è€…..."
æ–‡æœ¬2: "PY26 æ­£å¼å•Ÿå‹•ï¼ä½œç‚ºå‰µè¾¦äººç†äº‹æœƒé ˜è¢–..."

åºåˆ—ç›¸ä¼¼åº¦: 0.75 (å­—ç¬¦åºåˆ—åŒ¹é…)
å…±åŒå­—ç¬¦: 0.82 (å…±æœ‰å­—ç¬¦æ¯”ä¾‹)
è¯æ±‡é‡å : 0.68 (å…±åŒè¯æ±‡)

æœ€ç»ˆç›¸ä¼¼åº¦: 0.75 * 0.5 + 0.82 * 0.2 + 0.68 * 0.3 = 87.34%
```

#### 2. è´ªå©ªåŒ¹é…ç®—æ³•

```python
def smart_match_translations(old_table, new_texts, min_similarity=0.15):
    # æ­¥éª¤ 1: è®¡ç®—æ‰€æœ‰å¯èƒ½é…å¯¹çš„ç›¸ä¼¼åº¦
    pairings = []
    for i, old_row in enumerate(old_table):
        for j, new_text in enumerate(new_texts):
            similarity = calculate_text_similarity(old_row['target'], new_text)
            pairings.append((similarity, i, j))

    # æ­¥éª¤ 2: æŒ‰ç›¸ä¼¼åº¦ä»é«˜åˆ°ä½æ’åº
    pairings.sort(reverse=True, key=lambda x: x[0])

    # æ­¥éª¤ 3: è´ªå©ªé€‰æ‹©æœ€ä½³é…å¯¹
    used_old = set()
    used_new = set()
    matches = {}

    for similarity, old_idx, new_idx in pairings:
        if old_idx not in used_old and new_idx not in used_new:
            matches[old_idx] = (new_idx, similarity)
            used_old.add(old_idx)
            used_new.add(new_idx)

    return matches
```

**å·¥ä½œæµç¨‹**ï¼š
```
1. è®¡ç®—æ‰€æœ‰å¯èƒ½é…å¯¹:
   æ—§ç¿»è¯‘1 vs æ–°ç¿»è¯‘1: 45%
   æ—§ç¿»è¯‘1 vs æ–°ç¿»è¯‘2: 87%  â† æœ€é«˜
   æ—§ç¿»è¯‘1 vs æ–°ç¿»è¯‘3: 23%
   æ—§ç¿»è¯‘2 vs æ–°ç¿»è¯‘1: 82%  â† æ¬¡é«˜
   ...

2. æ’åº:
   (87%, æ—§1, æ–°2)
   (82%, æ—§2, æ–°1)
   (75%, æ—§3, æ–°3)
   ...

3. è´ªå©ªé€‰æ‹©:
   ç¬¬1è½®: é€‰æ‹© (87%, æ—§1, æ–°2) â†’ æ—§1å’Œæ–°2æ ‡è®°ä¸ºå·²ä½¿ç”¨
   ç¬¬2è½®: é€‰æ‹© (82%, æ—§2, æ–°1) â†’ æ—§2å’Œæ–°1æ ‡è®°ä¸ºå·²ä½¿ç”¨
   ç¬¬3è½®: é€‰æ‹© (75%, æ—§3, æ–°3) â†’ æ—§3å’Œæ–°3æ ‡è®°ä¸ºå·²ä½¿ç”¨
   ...
```

### ç›¸ä¼¼åº¦é˜ˆå€¼

é»˜è®¤æœ€å°ç›¸ä¼¼åº¦é˜ˆå€¼ï¼š**15%**

**ä½äºé˜ˆå€¼æ—¶ä¼šè­¦å‘Š**ï¼š
```
âš ï¸ è­¦å‘Šï¼š2 ä¸ªé…å¯¹çš„ç›¸ä¼¼åº¦è¾ƒä½ï¼ˆ< 15%ï¼‰
   å»ºè®®æ£€æŸ¥è¿™äº›é…å¯¹æ˜¯å¦æ­£ç¡®
```

**è°ƒæ•´é˜ˆå€¼**ï¼š

ç¼–è¾‘ `generate_translation_mapping.py`ï¼š
```python
new_translations = smart_match_translations(
    old_table,
    text_list,
    min_similarity=0.20,  # æé«˜åˆ° 20%
    verbose=args.verbose
)
```

- **æé«˜é˜ˆå€¼**ï¼ˆå¦‚ 0.30ï¼‰ï¼šæ›´ä¸¥æ ¼ï¼Œåªæ¥å—é«˜ç›¸ä¼¼åº¦é…å¯¹
- **é™ä½é˜ˆå€¼**ï¼ˆå¦‚ 0.10ï¼‰ï¼šæ›´å®½æ¾ï¼Œæ¥å—æ›´å¤šé…å¯¹

### é€‚ç”¨åœºæ™¯

âœ… **æœ€é€‚åˆ**ï¼š
- æ–°ç¿»è¯‘é¡ºåºä¸è¡¨æ ¼ä¸ä¸€è‡´
- ç¿»è¯‘é£æ ¼æœ‰è¾ƒå¤§å˜åŒ–ä½†å†…å®¹ç›¸å…³
- ä¸ç¡®å®šé¡ºåºæ˜¯å¦æ­£ç¡®

âŒ **ä¸é€‚åˆ**ï¼š
- å®Œå…¨ä¸ç›¸å…³çš„æ–‡æœ¬ï¼ˆç›¸ä¼¼åº¦ä¼šå¾ˆä½ï¼‰
- é«˜åº¦é‡å¤çš„æ–‡æœ¬ï¼ˆéš¾ä»¥åŒºåˆ†ï¼‰

---

## å ä½ç¬¦è‡ªåŠ¨è¿‡æ»¤

è‡ªåŠ¨è¯†åˆ«å¹¶è·³è¿‡å ä½ç¬¦è¡Œï¼Œé¿å…é”™è¯¯é…å¯¹ã€‚

### è¯†åˆ«è§„åˆ™

```python
def is_placeholder_row(text: str) -> bool:
    # è§„åˆ™ 1: ç§»é™¤å ä½ç¬¦å’Œå¸¸è§è¯åï¼Œå‰©ä½™å†…å®¹å¾ˆå°‘
    without_placeholders = re.sub(r'[<"]?\d+/?[>"]?', '', text)
    without_placeholders = re.sub(r'["""\'\'<>]', '', without_placeholders)
    without_placeholders = re.sub(r'(åœ¨ç¬¬|é |on page|page)', '', without_placeholders, flags=re.IGNORECASE)

    if len(without_placeholders.strip()) <= 3:
        return True

    # è§„åˆ™ 2: åŒ…å«å¤šä¸ªå ä½ç¬¦ä¸”æ–‡æœ¬å¾ˆçŸ­
    placeholder_count = len(re.findall(r'<\d+/>', text))
    if placeholder_count >= 2 and len(text) <= 30:
        return True

    return False
```

### è¯†åˆ«ç¤ºä¾‹

âœ… **ä¼šè¢«è¿‡æ»¤çš„å ä½ç¬¦è¡Œ**ï¼š
```
"<0/>"åœ¨ç¬¬ <1/> é         â†’ æ˜¯å ä½ç¬¦
"<2/>"                     â†’ æ˜¯å ä½ç¬¦
ç¬¬ <12/> é                 â†’ æ˜¯å ä½ç¬¦
å…§æ–‡                        â†’ æ˜¯å ä½ç¬¦ï¼ˆå¤ªçŸ­ï¼‰
<0/> on page <1/>          â†’ æ˜¯å ä½ç¬¦
```

âŒ **ä¸ä¼šè¢«è¿‡æ»¤çš„å®é™…å†…å®¹**ï¼š
```
PY26 æ­£å¼å•Ÿå‹•ï¼ä½œç‚ºå‰µè¾¦äººç†äº‹æœƒé ˜è¢–...  â†’ ä¸æ˜¯å ä½ç¬¦
æ‚¨æ˜¯åœ˜éšŠçš„æ¦œæ¨£ã€‚ç‚ºå”åŠ©æ‚¨æ›´è¼•é¬†ä¸”...      â†’ ä¸æ˜¯å ä½ç¬¦
è«‹è†è½å®‰éº—å¸‚å ´äº‹æ¥­ç¸½è£ John Parker... â†’ ä¸æ˜¯å ä½ç¬¦
```

### è¿‡æ»¤è¿‡ç¨‹

```
åŸå§‹è¡¨æ ¼: 26 è¡Œ
  â†“
åº”ç”¨è¿‡æ»¤è§„åˆ™
  â†“
å ä½ç¬¦è¡Œ: 13 è¡Œ (è·³è¿‡)
â”œâ”€ "<0/>"åœ¨ç¬¬ <1/> é 
â”œâ”€ "<2/>"
â”œâ”€ ç¬¬ <12/> é 
â”œâ”€ å…§æ–‡
â””â”€ ...
  â†“
ä¿ç•™è¡Œ: 13 è¡Œ (ç”¨äºåŒ¹é…)
â”œâ”€ PY26 æ­£å¼å•Ÿå‹•ï¼...
â”œâ”€ æ‚¨æ˜¯åœ˜éšŠçš„æ¦œæ¨£...
â””â”€ ...
```

### ç¦ç”¨è¿‡æ»¤ï¼ˆä¸æ¨èï¼‰

```bash
python3 ../scripts/generate_translation_mapping.py \
  --markdown "table.md" \
  --new-translations "new_trans.txt" \
  --output "translations.json" \
  --skip-placeholder-filter  # ä¸æ¨è
```

---

## è¿½è¸ªä¿®è®¢å¤„ç†

è‡ªåŠ¨æ£€æµ‹å’Œå¤„ç†å·²æœ‰è¿½è¸ªä¿®è®¢çš„ Word æ–‡æ¡£ã€‚

### é—®é¢˜èƒŒæ™¯

python-docx çš„é™åˆ¶ï¼š
```python
# æ™®é€šæ–‡æœ¬å¯ä»¥è¯»å–
for run in paragraph.runs:
    text = run.text  # âœ“ å¯ä»¥è¯»å–

# è¿½è¸ªä¿®è®¢ä¸­çš„æ–‡æœ¬æ— æ³•è¯»å–
# <w:del><w:r><w:delText>æ–‡æœ¬</w:delText></w:r></w:del>
for run in paragraph.runs:
    text = run.text  # âœ— runs ä¸ºç©ºï¼Œæ— æ³•è¯»å–
```

### è§£å†³æ–¹æ¡ˆï¼šç›´æ¥è§£æ XML

```python
def get_cell_text_from_tracked_changes(cell, mode='auto'):
    text_parts = []

    for paragraph in cell.paragraphs:
        para_element = paragraph._element

        if mode == 'read_deleted' or mode == 'auto':
            # è¯»å– <w:delText>
            del_elements = para_element.findall(qn('w:del'))
            for del_elem in del_elements:
                del_texts = del_elem.findall('.//' + qn('w:delText'))
                for del_text in del_texts:
                    if del_text.text:
                        text_parts.append(del_text.text)

        if mode == 'read_inserted' or mode == 'auto':
            # è¯»å– <w:ins> ä¸­çš„ <w:t>
            ins_elements = para_element.findall(qn('w:ins'))
            for ins_elem in ins_elements:
                ins_texts = ins_elem.findall('.//' + qn('w:t'))
                for ins_text in ins_texts:
                    if ins_text.text:
                        text_parts.append(ins_text.text)

    return ''.join(text_parts)
```

### ä¸‰ç§è¯»å–æ¨¡å¼

#### autoï¼ˆè‡ªåŠ¨æ£€æµ‹ï¼‰

```python
def auto_detect_text_source(cell):
    # 1. æ£€æŸ¥æ˜¯å¦æœ‰åˆ é™¤çš„æ–‡æœ¬
    deleted_text = get_cell_text_from_tracked_changes(cell, 'read_deleted')
    if deleted_text:
        return 'deleted', deleted_text

    # 2. æ£€æŸ¥æ˜¯å¦æœ‰æ’å…¥çš„æ–‡æœ¬
    inserted_text = get_cell_text_from_tracked_changes(cell, 'read_inserted')
    if inserted_text:
        return 'inserted', inserted_text

    # 3. è¯»å–æ™®é€šæ–‡æœ¬
    normal_text = get_cell_text_normal(cell)
    return 'normal', normal_text
```

**å†³ç­–æ ‘**ï¼š
```
æ£€æŸ¥å•å…ƒæ ¼
  â†“
æœ‰ <w:del> å—ï¼Ÿ
  â†“ æ˜¯
  è¯»å– <w:delText>
  â†“
  è¿”å› deleted

  â†“ å¦
æœ‰ <w:ins> å—ï¼Ÿ
  â†“ æ˜¯
  è¯»å– <w:ins> ä¸­çš„ <w:t>
  â†“
  è¿”å› inserted

  â†“ å¦
è¯»å–æ™®é€š runs
  â†“
  è¿”å› normal
```

#### read_deleted

å¼ºåˆ¶ä»åˆ é™¤çš„æ–‡æœ¬è¯»å–ï¼ˆ`<w:delText>`ï¼‰ã€‚

**é€‚ç”¨åœºæ™¯**ï¼š
- æ–‡æ¡£å·²æœ‰è¿½è¸ªä¿®è®¢
- æ—§ç¿»è¯‘åœ¨åˆ é™¤çš„æ–‡æœ¬ä¸­
- auto æ¨¡å¼é€‰æ‹©äº†é”™è¯¯çš„æ¥æº

#### read_inserted

å¼ºåˆ¶ä»æ’å…¥çš„æ–‡æœ¬è¯»å–ï¼ˆ`<w:ins>` ä¸­çš„ `<w:t>`ï¼‰ã€‚

**é€‚ç”¨åœºæ™¯**ï¼š
- æ–‡æ¡£å·²æœ‰è¿½è¸ªä¿®è®¢
- æ—§ç¿»è¯‘åœ¨æ’å…¥çš„æ–‡æœ¬ä¸­
- auto æ¨¡å¼é€‰æ‹©äº†é”™è¯¯çš„æ¥æº

### æ¸…é™¤å¹¶é‡æ–°åº”ç”¨

```python
def clear_and_apply_tracked_change(cell, old_text, new_text, author):
    # æ­¥éª¤ 1: æ¸…é™¤å•å…ƒæ ¼æ‰€æœ‰å†…å®¹
    for paragraph in cell.paragraphs:
        paragraph.clear()

    # æ­¥éª¤ 2: æ·»åŠ åˆ é™¤çš„æ—§æ–‡æœ¬
    para = cell.paragraphs[0]
    del_run = para._element.add_w_del()
    del_run.set(qn('w:author'), author)
    del_run.set(qn('w:date'), datetime.now().isoformat())

    del_r = del_run.add_w_r()
    del_text = del_r.add_w_delText()
    del_text.text = old_text

    # æ­¥éª¤ 3: æ·»åŠ æ’å…¥çš„æ–°æ–‡æœ¬
    ins_run = para._element.add_w_ins()
    ins_run.set(qn('w:author'), author)
    ins_run.set(qn('w:date'), datetime.now().isoformat())

    ins_r = ins_run.add_w_r()
    ins_text = ins_r.add_w_t()
    ins_text.text = new_text
```

**æ•ˆæœ**ï¼š
```xml
<!-- åº”ç”¨åçš„ XML ç»“æ„ -->
<w:p>
  <w:del w:author="Gemini" w:date="2025-01-06T10:30:00">
    <w:r>
      <w:delText>PY26 å·²è‡³ï¼Œä½œç‚ºå…¨çƒæ”¿ç­–è«®è©¢å§”å“¡...</w:delText>
    </w:r>
  </w:del>
  <w:ins w:author="Gemini" w:date="2025-01-06T10:30:00">
    <w:r>
      <w:t>PY26 æ­£å¼å•Ÿå‹•ï¼ä½œç‚ºå‰µè¾¦äººç†äº‹æœƒ...</w:t>
    </w:r>
  </w:ins>
</w:p>
```

**åœ¨ Word ä¸­æ˜¾ç¤º**ï¼š
```
PY26 å·²è‡³ï¼Œä½œç‚ºå…¨çƒæ”¿ç­–è«®è©¢å§”å“¡...  â† çº¢è‰²åˆ é™¤çº¿
PY26 æ­£å¼å•Ÿå‹•ï¼ä½œç‚ºå‰µè¾¦äººç†äº‹æœƒ...  â† çº¢è‰²ä¸‹åˆ’çº¿
```

---

## è¯Šæ–­å·¥å…·

æ·±åº¦åˆ†æ Word æ–‡æ¡£ç»“æ„ï¼Œè¯†åˆ«é—®é¢˜ï¼Œæä¾›è§£å†³æ–¹æ¡ˆå»ºè®®ã€‚

### åˆ†æå†…å®¹

```python
def analyze_cell_deep(cell):
    analysis = {
        'runs': [],
        'total_chars': 0,
        'has_tracked_changes': False,
        'xml_structure': ''
    }

    # 1. åˆ†æ runs
    for run in cell.paragraphs[0].runs:
        run_info = {
            'text': run.text,
            'style': run.style.name if run.style else None,
            'bold': run.bold,
            'italic': run.italic,
            'font_name': run.font.name,
            'font_size': run.font.size
        }
        analysis['runs'].append(run_info)

    # 2. æ£€æŸ¥è¿½è¸ªä¿®è®¢
    para_element = cell.paragraphs[0]._element
    if para_element.findall(qn('w:del')) or para_element.findall(qn('w:ins')):
        analysis['has_tracked_changes'] = True

    # 3. å¯¼å‡º XML
    analysis['xml_structure'] = etree.tostring(
        para_element,
        encoding='unicode',
        pretty_print=True
    )

    return analysis
```

### è‡ªåŠ¨å»ºè®®

```python
def generate_solution_recommendation(analysis):
    if analysis['has_tracked_changes']:
        return """
        æ¨èè§£å†³æ–¹æ¡ˆ:
          â†’ ä½¿ç”¨ update_fc_insider_tracked.py
          â†’ æ¨¡å¼: autoï¼ˆè‡ªåŠ¨æ£€æµ‹ï¼‰
          â†’ æˆ–æ ¹æ® XML ç»“æ„é€‰æ‹© read_deleted/read_inserted
        """

    if len(analysis['runs']) == 0:
        return """
        é—®é¢˜: æ— æ³•è¯»å– runs
        å¯èƒ½åŸå› : è¿½è¸ªä¿®è®¢æˆ–ç‰¹æ®Šæ ¼å¼
        å»ºè®®: ä½¿ç”¨ update_fc_insider_tracked.py
        """

    return """
    æ¨èè§£å†³æ–¹æ¡ˆ:
      â†’ ä½¿ç”¨ update_fc_insider_simple.py
      â†’ æ–‡æ¡£ç»“æ„æ­£å¸¸
    """
```

### ä½¿ç”¨ç¤ºä¾‹

```bash
# åŸºæœ¬åˆ†æ
python3 ../scripts/analyze_word_structure_deep.py \
  --input "input.docx" \
  --sample-segment "1360baf04e-73fb-432d-abf1-a0887de5f16a" \
  --verbose

# å¯¼å‡º XML å’Œ JSON
python3 ../scripts/analyze_word_structure_deep.py \
  --input "input.docx" \
  --sample-segment "1360baf04e-73fb-432d-abf1-a0887de5f16a" \
  --export-xml \
  --export-json "analysis.json" \
  --verbose
```

---

## è‡ªåŠ¨æ–‡æœ¬è½¬ JSON

ä½¿ç”¨çº¯æ–‡æœ¬ + segment_id åŒ¹é…æ—¶ï¼Œè‡ªåŠ¨å°†æ–‡æœ¬è½¬æ¢ä¸º JSON æ ¼å¼ã€‚

### å·¥ä½œåŸç†

```python
def auto_convert_text_to_json(text_dict, old_table):
    """
    å°†çº¯æ–‡æœ¬è½¬æ¢ä¸º segment_id æ˜ å°„

    text_dict: {"0": "ç¬¬1è¡Œ", "1": "ç¬¬2è¡Œ", ...}
    old_table: [{"segment_id": "abc", ...}, ...]

    è¿”å›: {"abc": "ç¬¬1è¡Œ", "def": "ç¬¬2è¡Œ", ...}
    """
    json_dict = {}

    for idx, row in enumerate(old_table):
        segment_id = row['segment_id']
        text = text_dict.get(str(idx))

        if text:
            json_dict[segment_id] = text

    return json_dict
```

### ä½¿ç”¨ç¤ºä¾‹

```bash
# çº¯æ–‡æœ¬æ–‡ä»¶ + segment_id åŒ¹é…
python3 ../scripts/generate_translation_mapping.py \
  --markdown "table.md" \
  --new-translations "new_trans.txt" \  # çº¯æ–‡æœ¬ï¼
  --output "translations.json" \
  --match-by segment_id \  # segment_id åŒ¹é…ï¼
  --verbose
```

**è¾“å‡º**ï¼š
```
ğŸ”„ æ£€æµ‹åˆ°çº¯æ–‡æœ¬æ ¼å¼ + segment_id åŒ¹é…æ¨¡å¼
   è‡ªåŠ¨å°†æ–‡æœ¬è½¬æ¢ä¸º JSON æ ¼å¼ï¼ˆæ–‡æœ¬è¡Œ â†’ segment_idï¼‰...
âœ“ è½¬æ¢å®Œæˆï¼š13 ä¸ªè¯‘æ–‡å·²æ˜ å°„åˆ° segment_id

è½¬æ¢ç¤ºä¾‹ï¼ˆå‰3ä¸ªï¼‰:
  1. 1360baf04e...: PY26 æ­£å¼å•Ÿå‹•ï¼...
  2. 1460baf04e...: æ‚¨æ˜¯åœ˜éšŠçš„æ¦œæ¨£...
  3. 1500986be2...: è«‹è†è½å®‰éº—å¸‚å ´äº‹æ¥­ç¸½è£...
```

---

## è‡ªå®šä¹‰ç›¸ä¼¼åº¦ç®—æ³•

å¦‚æœé»˜è®¤çš„ç›¸ä¼¼åº¦ç®—æ³•ä¸é€‚åˆä½ çš„åœºæ™¯ï¼Œå¯ä»¥è‡ªå®šä¹‰ã€‚

### ä¿®æ”¹æƒé‡

ç¼–è¾‘ `generate_translation_mapping.py`ï¼š

```python
def calculate_text_similarity(text1: str, text2: str) -> float:
    seq_ratio = SequenceMatcher(None, text1, text2).ratio()

    set1, set2 = set(text1), set(text2)
    char_ratio = len(set1 & set2) / max(len(set1), len(set2)) if set1 and set2 else 0

    words1 = set(re.findall(r'[\w]+', text1))
    words2 = set(re.findall(r'[\w]+', text2))
    word_ratio = len(words1 & words2) / max(len(words1), len(words2)) if words1 and words2 else 0

    # è°ƒæ•´æƒé‡
    # é»˜è®¤: 0.5, 0.2, 0.3
    # æ›´æ³¨é‡åºåˆ—: 0.7, 0.15, 0.15
    # æ›´æ³¨é‡è¯æ±‡: 0.3, 0.2, 0.5
    return (seq_ratio * 0.5) + (char_ratio * 0.2) + (word_ratio * 0.3)
```

### æ·»åŠ æ–°çš„ç›¸ä¼¼åº¦æ–¹æ³•

```python
def calculate_text_similarity_custom(text1: str, text2: str) -> float:
    # æ–¹æ³• 1-3: ç°æœ‰æ–¹æ³•
    ...

    # æ–¹æ³• 4: è‡ªå®šä¹‰æ–¹æ³•ï¼ˆä¾‹å¦‚ï¼šé•¿åº¦ç›¸ä¼¼åº¦ï¼‰
    len_ratio = min(len(text1), len(text2)) / max(len(text1), len(text2))

    # ç»¼åˆ
    return (seq_ratio * 0.4) + (char_ratio * 0.2) + (word_ratio * 0.3) + (len_ratio * 0.1)
```

---

## æ€»ç»“

æŒæ¡è¿™äº›é«˜çº§åŠŸèƒ½ï¼Œå¯ä»¥å¤„ç†æ›´å¤æ‚çš„åœºæ™¯ï¼š

- **æ™ºèƒ½åŒ¹é…** - å¤„ç†é¡ºåºä¸ä¸€è‡´
- **å ä½ç¬¦è¿‡æ»¤** - è‡ªåŠ¨è·³è¿‡æ— ç”¨è¡Œ
- **è¿½è¸ªä¿®è®¢å¤„ç†** - å¤„ç†å·²æœ‰ä¿®è®¢çš„æ–‡æ¡£
- **è¯Šæ–­å·¥å…·** - å¿«é€Ÿå®šä½é—®é¢˜
- **è‡ªåŠ¨è½¬æ¢** - ç®€åŒ–æ–‡ä»¶å‡†å¤‡

æ ¹æ®å®é™…éœ€æ±‚é€‰æ‹©åˆé€‚çš„åŠŸèƒ½å’Œå‚æ•°ï¼
